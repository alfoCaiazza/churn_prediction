{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c40c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import dagshub\n",
    "from dagshub import dagshub_logger\n",
    "import keras_tuner\n",
    "from keras_tuner import BayesianOptimization\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be9dbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3333 entries, 0 to 3332\n",
      "Data columns (total 15 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   AccountWeeks         3333 non-null   float64\n",
      " 1   DataUsage            3333 non-null   float64\n",
      " 2   CustServCalls        3333 non-null   float64\n",
      " 3   DayMins              3333 non-null   float64\n",
      " 4   DayCalls             3333 non-null   float64\n",
      " 5   MonthlyCharge        3333 non-null   float64\n",
      " 6   OverageFee           3333 non-null   float64\n",
      " 7   RoamMins             3333 non-null   float64\n",
      " 8   AvgMinPerCall        3333 non-null   float64\n",
      " 9   AvgDataUsagePerWeek  3333 non-null   float64\n",
      " 10  RoamMinsRatio        3333 non-null   float64\n",
      " 11  ComplaintIndex       3333 non-null   float64\n",
      " 12  Churn                3333 non-null   int64  \n",
      " 13  ContractRenewal      3333 non-null   int64  \n",
      " 14  DataPlan             3333 non-null   int64  \n",
      "dtypes: float64(12), int64(3)\n",
      "memory usage: 390.7 KB\n"
     ]
    }
   ],
   "source": [
    "DF_PATH = \"../data/dataframes/telecom_churn_scaled.csv\"\n",
    "df = pd.read_csv(DF_PATH)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa30890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Churn'])\n",
    "y = df['Churn']\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, shuffle=True, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=(15/85), shuffle=True, stratify=y_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1125a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining MLP model\n",
    "def build_model(hp):\n",
    "    # Input layer\n",
    "    input_layer = keras.layers.Input(shape=(14,), dtype=float)\n",
    "    x = input_layer\n",
    "\n",
    "    # Dense layers\n",
    "    activation = hp.Choice('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "    batch_norm = hp.Boolean('batch_norm')\n",
    "\n",
    "    # Dense Layers \n",
    "    x = keras.layers.Dense(units=256, activation=activation)(x)\n",
    "    if batch_norm:\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = keras.layers.Dense(units=128, activation=activation)(x)\n",
    "    if batch_norm:\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = keras.layers.Dense(units=64, activation=activation)(x)\n",
    "    if batch_norm:\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = keras.layers.Dense(units=32, activation=activation)(x)\n",
    "    if batch_norm:\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Defyning the optimizer\n",
    "    optimizer = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            learning_rate=hp.Float('adam_lr', 1e-5, 1e-2, sampling='log')\n",
    "    )\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = keras.optimizers.RMSprop(\n",
    "            learning_rate=hp.Float('rmsprop_lr', 1e-5, 1e-2, sampling='log'),\n",
    "            rho=hp.Float('rmsprop_rho', 0.8, 0.99)\n",
    "    )\n",
    "    elif optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(\n",
    "            learning_rate=hp.Float('sgd_lr', 1e-4, 1e-1, sampling='log'),\n",
    "            momentum=hp.Float('sgd_momentum', 0.0, 0.99)\n",
    "    )\n",
    "        \n",
    "    # Instantiating the model\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            keras.metrics.AUC(name='auc_pr', curve='PR'), # best-practice for a rare positive class\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c3f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.5, min_lr=1e-5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c2eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 17s]\n",
      "val_auc_pr: 0.42211705446243286\n",
      "\n",
      "Best val_auc_pr So Far: 0.6892575025558472\n",
      "Total elapsed time: 00h 03m 34s\n"
     ]
    }
   ],
   "source": [
    "# Instantiating keras tuner\n",
    "tuner = BayesianOptimization(\n",
    "    hypermodel= build_model,\n",
    "    objective= keras_tuner.Objective('val_auc_pr', direction='max'), # Same objective as the model itself\n",
    "    max_trials=10,\n",
    "    seed=42,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_trials',\n",
    "    project_name='mlp_churn_prediction',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Defining class weights to prevent unbalanced predictions\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Starting the optimization\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size = 64,\n",
    "    callbacks = base_callbacks,\n",
    "    class_weight=class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b604c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation : relu\n",
      "dropout_rate : 0.1\n",
      "batch_norm : True\n",
      "optimizer : sgd\n",
      "adam_lr : 0.005361764238699136\n",
      "rmsprop_lr : 9.359659238480389e-05\n",
      "rmsprop_rho : 0.813951064412443\n",
      "sgd_lr : 0.0006909496810943752\n",
      "sgd_momentum : 0.8511311607399468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acaia/miniconda3/envs/template_env/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'SGD', because it has 2 variables whereas the saved optimizer has 20 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "for param, value in best_hps.values.items():\n",
    "    print(f\"{param} : {value}\")\n",
    "\n",
    "# Saving the model\n",
    "model_path = \"../models/mlp/mlp_param2.keras\"\n",
    "best_model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36533440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - auc_pr: 0.6654 - loss: 0.3691 - precision: 0.5155 - recall: 0.6944\n",
      "Evaluation results: [0.3691120743751526, 0.6653565168380737, 0.5154638886451721, 0.6944444179534912]\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "results = best_model.evaluate(X_test, y_test)\n",
    "print(\"Evaluation results:\", results)\n",
    "\n",
    "y_probs = best_model.predict(X_test)\n",
    "y_pred = (y_probs > 0.5).astype(int) # Label assignement for a binary task\n",
    "\n",
    "y_true = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b981a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report {\n",
      "    \"0\": {\n",
      "        \"precision\": 0.9454094292803971,\n",
      "        \"recall\": 0.8901869158878505,\n",
      "        \"f1-score\": 0.9169675090252708,\n",
      "        \"support\": 428.0\n",
      "    },\n",
      "    \"1\": {\n",
      "        \"precision\": 0.5154639175257731,\n",
      "        \"recall\": 0.6944444444444444,\n",
      "        \"f1-score\": 0.591715976331361,\n",
      "        \"support\": 72.0\n",
      "    },\n",
      "    \"accuracy\": 0.862,\n",
      "    \"macro avg\": {\n",
      "        \"precision\": 0.7304366734030852,\n",
      "        \"recall\": 0.7923156801661475,\n",
      "        \"f1-score\": 0.7543417426783159,\n",
      "        \"support\": 500.0\n",
      "    },\n",
      "    \"weighted avg\": {\n",
      "        \"precision\": 0.8834972755877313,\n",
      "        \"recall\": 0.862,\n",
      "        \"f1-score\": 0.8701312883173478,\n",
      "        \"support\": 500.0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_true=y_true, y_pred=y_pred, output_dict=True, zero_division=0)\n",
    "print(\"Classification Report\", json.dumps(report, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "template_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
